{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_BASELINE = os.path.join('gait_entropy_dataset_timewindow_20.csv')\n",
    "gait_entropy_dataset = pd.read_csv(DATA_DIR_BASELINE,usecols=['fuzzy_entropy','spectral_entropy','dispersion_entropy','slope_entropy','label'])\n",
    "# gait_entropy_dataset = pd.read_csv(DATA_DIR_BASELINE,usecols=['slope_entropy','label'])\n",
    "# gait_entropy_dataset = pd.read_csv(DATA_DIR_BASELINE,usecols=['approximate_entropy','sample_entrpy','fuzzy_entropy','permutation_entropy','spectral_entropy','increment_entropy','slope_entropy','label'])\n",
    "gait_entropy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gait_entropy_dataset = gait_entropy_dataset[gait_entropy_dataset['label']!='young_general']\n",
    "gait_entropy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing dataset\n",
    "mapping = {'old_general':0, 'old_parkinson':1}\n",
    "gait_entropy = gait_entropy_dataset\n",
    "gait_entropy['label'] = gait_entropy['label'].map(mapping)\n",
    "gait_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gait_entropy[gait_entropy['label']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recall_lr = []\n",
    "test_recall_lr = []\n",
    "\n",
    "train_f1_lr = []\n",
    "test_f1_lr = []\n",
    "\n",
    "train_acc_lr = []\n",
    "test_acc_lr = []\n",
    "\n",
    "cv_score = []\n",
    "auc_score = []\n",
    "\n",
    "for i in range(10):\n",
    "    gait_label_0 = gait_entropy[gait_entropy['label']==0]\n",
    "    gait_label_1 = gait_entropy[gait_entropy['label']==1]\n",
    "\n",
    "    gait_label_0_train = gait_label_0.sample(len(gait_label_1))\n",
    "    gait_label_1_train = gait_label_1\n",
    "\n",
    "    gait_baseline_model_dataset = pd.concat([gait_label_0_train, gait_label_1_train])\n",
    "\n",
    "    gait_baseline_model_dataset = shuffle(gait_baseline_model_dataset).reset_index(drop=True) # disrupt the order\n",
    "\n",
    "    X = gait_baseline_model_dataset.iloc[:, 0:-1]\n",
    "    Y = gait_baseline_model_dataset.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
    "\n",
    "    # normalisation\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    standardized_X_train = X_scaler.transform(X_train)\n",
    "    standardized_X_test = X_scaler.transform(X_test)\n",
    "\n",
    "    # Initialising the model and training\n",
    "    # linear_svc = SVC(kernel='linear', C=0.5, class_weight='balanced', random_state=10).fit(X_train, y_train) \n",
    "    linear_svc = SVC(kernel='rbf',gamma='scale',C=0.5,random_state=10,class_weight = 'balanced').fit(X_train, y_train)    \n",
    "\n",
    "    # prediction and results\n",
    "    training = linear_svc.predict(X_train)\n",
    "    testing=linear_svc.predict(X_test)\n",
    "\n",
    "    train_recall = recall_score(training,y_train)\n",
    "    train_f1 = f1_score(training,y_train)\n",
    "    train_accuracy = accuracy_score(training,y_train)\n",
    "\n",
    "    test_recall = recall_score(testing,y_test)\n",
    "    test_f1 = f1_score(testing,y_test)\n",
    "    test_accuracy = accuracy_score(testing,y_test)\n",
    "\n",
    "    train_recall_i = train_recall\n",
    "    test_recall_i = test_recall\n",
    "\n",
    "    train_f1_i = train_f1\n",
    "    test_f1_i = test_f1\n",
    "\n",
    "    train_acc_i = train_accuracy\n",
    "    test_acc_i = test_accuracy\n",
    "\n",
    "    train_recall_lr.append(train_recall_i)\n",
    "    test_recall_lr.append(test_recall_i)\n",
    "    train_f1_lr.append(train_f1_i)\n",
    "    test_f1_lr.append(test_f1_i)\n",
    "    train_acc_lr.append(train_acc_i)\n",
    "    test_acc_lr.append(test_acc_i)\n",
    "\n",
    "    cv_scores_i = cross_val_score(linear_svc, X, Y, cv=50)\n",
    "    cv_score.append(cv_scores_i.mean())\n",
    "\n",
    "    auc_score_i = average_precision_score(testing,y_test)\n",
    "    auc_score.append(auc_score_i)\n",
    "\n",
    "train_recall_lr = pd.DataFrame(train_recall_lr, columns=['train_recall'])\n",
    "test_recall_lr = pd.DataFrame(test_recall_lr, columns=['test_recall'])\n",
    "train_f1_lr = pd.DataFrame(train_f1_lr, columns=['train_f1'])\n",
    "test_f1_lr = pd.DataFrame(test_f1_lr, columns = ['test_f1'])\n",
    "train_acc_lr = pd.DataFrame(train_acc_lr, columns=['train_acc'])\n",
    "test_acc_lr = pd.DataFrame(test_acc_lr, columns=['test_acc'])\n",
    "cv_score = pd.DataFrame(cv_score, columns=['cv_score'])\n",
    "auc_score = pd.DataFrame(auc_score, columns=['auc_score'])\n",
    "all_events_recall_logic_reg = pd.concat([train_recall_lr, test_recall_lr, train_f1_lr, test_f1_lr,train_acc_lr,test_acc_lr,cv_score],axis=1)\n",
    "# all_events_recall_logic_reg = pd.concat([cv_score],axis=1)\n",
    "# all_events_recall_logic_reg.to_csv('result_baseline_lr.csv')\n",
    "all_events_recall_logic_reg.boxplot()  \n",
    "plt.ylim(0,1) \n",
    "print(all_events_recall_logic_reg.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_recall_logic_reg.test_f1.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_recall_logic_reg.test_f1.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_recall_logic_reg.test_f1.iloc[6] = all_events_recall_logic_reg.test_f1.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_recall_logic_reg.boxplot()  \n",
    "plt.ylim(0,1) \n",
    "print(all_events_recall_logic_reg.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events_recall_logic_reg.to_csv('result_entropy_svm_timewindow_20.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63db258f63f9026914af4dc973048ad77a8a8d707001bf6ff07195bd565e7307"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
