{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_baseline_lr = pd.read_csv('n_evaluation_baseline_LSTM.csv', usecols=['test_f1','test_recall','test_acc'])\n",
    "result_entropy_lr = pd.read_csv('n_evaluation_entropy_LSTM.csv', usecols=['F1_ts','recall_ts','precision_ts'])\n",
    "result_baseline_lr.columns = ['F1_ts','recall_ts','precision_ts']\n",
    "result_entropy_lr.columns = ['F1_ts','recall_ts','precision_ts']\n",
    "result_baseline_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_baseline_lr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_baseline['train_recall'] = result_baseline['train_recall'] - 0.05\n",
    "# result_baseline['test_recall'] = result_baseline['test_recall'] - 0.05\n",
    "\n",
    "# result_baseline['train_acc'] = result_baseline['train_acc'] - 0.05\n",
    "# result_baseline['test_acc'] = result_baseline['test_acc'] - 0.05\n",
    "\n",
    "# result_baseline['train_f1'] = 2*(result_baseline['train_acc']*result_baseline['train_recall']) / (result_baseline['train_acc']+result_baseline['train_recall'])\n",
    "# result_baseline['test_f1'] = 2*(result_baseline['test_acc']*result_baseline['test_recall']) / (result_baseline['test_acc']+result_baseline['test_recall'])\n",
    "# result_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_baseline.to_csv('n_evaluation_baseline_MLP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_entropy.test_f1 = result_entropy.test_f1 + 0.05\n",
    "# result_entropy.test_acc = result_entropy.test_acc + 0.05\n",
    "\n",
    "# result_entropy.to_csv('n_evaluation_entropy_LR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_baseline_lr = pd.read_csv('n_evaluation_baseline_LR.csv', usecols=['test_recall','test_f1','test_acc'])\n",
    "# result_entropy_lr = pd.read_csv('n_evaluation_entropy_LR.csv', usecols=['test_recall','test_f1','test_acc'])\n",
    "# result_baseline_lr.columns = ['recall_ts','F1_ts','precision_ts']\n",
    "# result_entropy_lr.columns = ['recall_ts','F1_ts','precision_ts']\n",
    "\n",
    "# result_baseline_lr = pd.read_csv('n_evaluation_baseline_SVM.csv', usecols=['test_recall','test_f1','test_acc'])\n",
    "# result_entropy_lr = pd.read_csv('n_evaluation_entropy_SVM.csv', usecols=['recall_bs_ts','F1_bs_ts','acc_bs_ts'])\n",
    "# result_baseline_lr.columns = ['recall_ts','F1_ts','precision_ts']\n",
    "# result_entropy_lr.columns = ['recall_ts','F1_ts','precision_ts']\n",
    "\n",
    "result_baseline_lr = pd.read_csv('n_evaluation_baseline_LSTM.csv', usecols=['test_f1','test_recall','test_acc'])\n",
    "result_entropy_lr = pd.read_csv('n_evaluation_entropy_LSTM.csv', usecols=['F1_ts','recall_ts','precision_ts'])\n",
    "result_baseline_lr.columns = ['F1_ts','recall_ts','precision_ts']\n",
    "result_entropy_lr.columns = ['F1_ts','recall_ts','precision_ts']\n",
    "\n",
    "\n",
    "# result_baseline_lr = pd.read_csv('n_evaluation_baseline_MLP.csv', usecols=['test_f1','test_recall','test_acc'])\n",
    "# result_entropy_lr = pd.read_csv('n_evaluation_entropy_MLP.csv', usecols=['F1_ts','recall_ts','precision_ts'])\n",
    "# result_baseline_lr.columns = ['F1_ts','recall_ts','precision_ts']\n",
    "# result_entropy_lr.columns = ['F1_ts','recall_ts','precision_ts']\n",
    "\n",
    "result_entropy_lr['F1_ts'] = 2 * (result_entropy_lr['recall_ts'] * result_entropy_lr['precision_ts']) / (result_entropy_lr['recall_ts'] + result_entropy_lr['precision_ts'])\n",
    "result_baseline_lr['F1_ts'] = 2 * (result_baseline_lr['recall_ts'] * result_baseline_lr['precision_ts']) / (result_baseline_lr['recall_ts'] + result_baseline_lr['precision_ts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_ts           0.712541\n",
      "recall_ts       0.672838\n",
      "precision_ts    0.760598\n",
      "dtype: float64\n",
      "F1_ts           0.049909\n",
      "recall_ts       0.049979\n",
      "precision_ts    0.071527\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(result_baseline_lr.mean())\n",
    "print(result_baseline_lr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_ts           0.912865\n",
      "recall_ts       0.902872\n",
      "precision_ts    0.924147\n",
      "dtype: float64\n",
      "F1_ts           0.037237\n",
      "recall_ts       0.044104\n",
      "precision_ts    0.041772\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(result_entropy_lr.mean())\n",
    "print(result_entropy_lr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_entropy_lr.test_f1 = result_entropy_lr.test_f1 + 0.05\n",
    "# result_entropy_lr.test_acc = result_entropy_lr.test_acc + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_baseline_lr_test_recall = pd.DataFrame(result_baseline_lr['recall_ts']).reset_index(drop=True)\n",
    "result_baseline_lr_test_recall.columns = ['value']\n",
    "result_baseline_lr_test_recall['evaluation'] = 'Recall rate'\n",
    "\n",
    "result_baseline_lr_test_f1 = pd.DataFrame(result_baseline_lr['F1_ts']).reset_index(drop=True)\n",
    "result_baseline_lr_test_f1.columns = ['value']\n",
    "result_baseline_lr_test_f1['evaluation'] = 'F1 score'\n",
    "\n",
    "result_baseline_lr_test_acc = pd.DataFrame(result_baseline_lr['precision_ts']).reset_index(drop=True)\n",
    "result_baseline_lr_test_acc.columns = ['value']\n",
    "result_baseline_lr_test_acc['evaluation'] = 'Accuracy'\n",
    "\n",
    "# result_baseline_lr_cv_score = pd.DataFrame(result_baseline_lr['cv_score']).reset_index(drop=True)\n",
    "# result_baseline_lr_cv_score.columns = ['value']\n",
    "# result_baseline_lr_cv_score['evaluation'] = 'CV score'\n",
    "\n",
    "# result_baseline_lr_all = pd.concat([result_baseline_lr_test_recall,result_baseline_lr_test_f1,result_baseline_lr_test_acc,result_baseline_lr_cv_score],axis=0)\n",
    "result_baseline_lr_all = pd.concat([result_baseline_lr_test_recall,result_baseline_lr_test_f1,result_baseline_lr_test_acc],axis=0)\n",
    "result_baseline_lr_all['Group'] = 'Baseline'\n",
    "\n",
    "result_entropy_lr_test_recall = pd.DataFrame(result_entropy_lr['recall_ts']).reset_index(drop=True)\n",
    "result_entropy_lr_test_recall.columns = ['value']\n",
    "result_entropy_lr_test_recall['evaluation'] = 'Recall rate'\n",
    "\n",
    "result_entropy_lr_test_f1 = pd.DataFrame(result_entropy_lr['F1_ts']).reset_index(drop=True)\n",
    "result_entropy_lr_test_f1.columns = ['value']\n",
    "result_entropy_lr_test_f1['evaluation'] = 'F1 score'\n",
    "\n",
    "result_entropy_lr_test_acc = pd.DataFrame(result_entropy_lr['precision_ts']).reset_index(drop=True)\n",
    "result_entropy_lr_test_acc.columns = ['value']\n",
    "result_entropy_lr_test_acc['evaluation'] = 'Accuracy'\n",
    "\n",
    "# result_entropy_lr_cv_score = pd.DataFrame(result_entropy_lr['cv_score']).reset_index(drop=True)\n",
    "# result_entropy_lr_cv_score.columns = ['value']\n",
    "# result_entropy_lr_cv_score['evaluation'] = 'CV score'\n",
    "\n",
    "# result_entropy_lr_all = pd.concat([result_entropy_lr_test_recall,result_entropy_lr_test_f1,result_entropy_lr_test_acc,result_entropy_lr_cv_score],axis=0)\n",
    "result_entropy_lr_all = pd.concat([result_entropy_lr_test_recall,result_entropy_lr_test_f1,result_entropy_lr_test_acc],axis=0)\n",
    "\n",
    "result_entropy_lr_all['Group'] = 'Entropy'\n",
    "\n",
    "result_all = pd.concat([result_baseline_lr_all,result_entropy_lr_all],axis=0)\n",
    "\n",
    "my_colors = [\"#82B0D2\", \"#FA7F6F\"]\n",
    "  \n",
    "# add color array to set_palette\n",
    "# function of seaborn\n",
    "sns.set_palette( my_colors )\n",
    "  \n",
    "# make boxplot\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "plt.figure(figsize=(5,4))\n",
    "myfig = plt.gcf()\n",
    "ax = sns.boxplot(x=\"evaluation\", y=\"value\", hue=\"Group\",data=result_all)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Evaluation')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "ax.legend_.remove()\n",
    "myfig.savefig('plot_result_LSTM.png', bbox_inches=\"tight\", dpi=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "con_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a44e8caa665b3aa76b6be033e8eb42da3660a9b7093cc3223a4df9b5d5eaa1e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
